{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c96d406-25c4-467c-9dcc-5a28b8a3c07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install koboextractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95af95c2-5825-446f-baf0-56cf86c65a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from koboextractor import KoboExtractor\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Kobo credentials and API setup\n",
    "api_token = '64deabcd2830d55dafc9a765cf2453c41fde07fc'  # Replace with secure storage in production\n",
    "form_id = 'aKhrYSrKMa6Qtfgzvfjnhn'\n",
    "kobo_base_url = 'https://kobo.humanitarianresponse.info/api/v2'\n",
    "\n",
    "# Initialize KoboExtractor\n",
    "kobo = KoboExtractor(api_token, kobo_base_url)\n",
    "\n",
    "# Function to fetch KoboToolbox data and return as a Spark DataFrame\n",
    "def fetch_kobo_to_spark(api_token, form_id, base_url):\n",
    "    try:\n",
    "        # Fetch data from KoboToolbox API\n",
    "        data = kobo.get_data(form_id)\n",
    "\n",
    "        if \"results\" not in data:\n",
    "            raise KeyError(\"'results' key not found in KoboToolbox API response.\")\n",
    "\n",
    "        # Flatten the JSON response\n",
    "        df = pd.json_normalize(data[\"results\"])\n",
    "\n",
    "        # Optional: Convert timestamp fields to datetime if present\n",
    "        if 'submission_time' in df.columns:\n",
    "            df['submission_time'] = pd.to_datetime(df['submission_time'])\n",
    "\n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        return spark_df\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to fetch or process data: {e}\")\n",
    "\n",
    "# Fetch data and convert to Spark DataFrame\n",
    "spark_df = fetch_kobo_to_spark(api_token, form_id, kobo_base_url)\n",
    "\n",
    "# Fix NullType arrays by casting to known types for Delta compatibility\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"_attachments\", col(\"_attachments\").cast(ArrayType(StringType()))\n",
    ").withColumn(\n",
    "    \"_geolocation\", col(\"_geolocation\").cast(ArrayType(DoubleType()))\n",
    ").withColumn(\n",
    "    \"_tags\", col(\"_tags\").cast(ArrayType(StringType()))\n",
    ").withColumn(\n",
    "    \"_notes\", col(\"_notes\").cast(ArrayType(StringType()))\n",
    ")\n",
    "\n",
    "# Write to Delta table\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"kobo_submission\")\n",
    "\n",
    "# Optionally display a preview\n",
    "display(spark_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7522cb41-bbd5-4876-b4bf-b0e679aa91b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from koboextractor import KoboExtractor\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Kobo credentials and API setup\n",
    "api_token = '64deabcd2830d55dafc9a765cf2453c41fde07fc'  # Replace with secure storage in production\n",
    "form_id = 'aKhrYSrKMa6Qtfgzvfjnhn'\n",
    "kobo_base_url = 'https://kobo.humanitarianresponse.info/api/v2'\n",
    "\n",
    "# Initialize KoboExtractor\n",
    "kobo = KoboExtractor(api_token, kobo_base_url)\n",
    "\n",
    "# Function to fetch KoboToolbox data and return as a Spark DataFrame\n",
    "def fetch_kobo_to_spark(api_token, form_id, base_url):\n",
    "    try:\n",
    "        # Fetch data from KoboToolbox API\n",
    "        data = kobo.get_data(form_id)\n",
    "\n",
    "        if \"results\" not in data:\n",
    "            raise KeyError(\"'results' key not found in KoboToolbox API response.\")\n",
    "\n",
    "        # Flatten the JSON response\n",
    "        df = pd.json_normalize(data[\"results\"])\n",
    "\n",
    "        # Optional: Convert timestamp fields to datetime if present\n",
    "        if 'submission_time' in df.columns:\n",
    "            df['submission_time'] = pd.to_datetime(df['submission_time'])\n",
    "\n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        return spark_df\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to fetch or process data: {e}\")\n",
    "\n",
    "# Function to continuously ingest data\n",
    "def continuous_data_ingestion(api_token, form_id, base_url, interval=300):\n",
    "    while True:\n",
    "        try:\n",
    "            # Fetch data and convert to Spark DataFrame\n",
    "            spark_df = fetch_kobo_to_spark(api_token, form_id, base_url)\n",
    "\n",
    "            # Fix NullType arrays by casting to known types for Delta compatibility\n",
    "            spark_df = spark_df.withColumn(\n",
    "                \"_attachments\", col(\"_attachments\").cast(ArrayType(StringType()))\n",
    "            ).withColumn(\n",
    "                \"_geolocation\", col(\"_geolocation\").cast(ArrayType(DoubleType()))\n",
    "            ).withColumn(\n",
    "                \"_tags\", col(\"_tags\").cast(ArrayType(StringType()))\n",
    "            ).withColumn(\n",
    "                \"_notes\", col(\"_notes\").cast(ArrayType(StringType()))\n",
    "            )\n",
    "\n",
    "            # Write to Delta table\n",
    "            spark_df.write.mode(\"append\").saveAsTable(\"kobo_submission\")\n",
    "\n",
    "            # Optional: Display a preview of the latest data\n",
    "            display(spark_df.limit(10))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ingestion: {e}\")\n",
    "\n",
    "        # Wait for the next iteration\n",
    "        time.sleep(interval)  # Wait for 'interval' seconds before fetching data again\n",
    "\n",
    "# Start the continuous data ingestion process (set to fetch every 5 minutes)\n",
    "continuous_data_ingestion(api_token, form_id, kobo_base_url, interval=300)  # interval in seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775480fd-d9d8-4228-8032-973dfd6b1dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver table \n",
    "\n",
    "\n",
    "# Silver Layer: Refined Data (data cleaning and transformation)\n",
    "silver_df = spark_df.select(\n",
    "    'submission_time',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'deviceid',\n",
    "    'formhub/uuid',\n",
    "    'meta/instanceID',\n",
    "    'formhub/submitter_id',\n",
    "    'location',\n",
    "    'other_fields'  # Add specific fields you need\n",
    ")\n",
    "\n",
    "# Handle missing or malformed data\n",
    "silver_df = silver_df.na.drop(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Correcting the column name for submission time\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"_submission_time\", \n",
    "    col(\"submission_time\").cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Optional: Enrich the data (example: adding a new column based on logic)\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"is_valid_location\", \n",
    "    (col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull()).cast(\"boolean\")\n",
    ")\n",
    "\n",
    "# Now you can proceed with further transformations or saving to Delta\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kobo_silver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc57538-7fcf-4154-89b9-1eaa6b1bb8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold Table\n",
    "\n",
    "# Gold Layer: Aggregated or Analytical Data (e.g., daily submissions, region counts)\n",
    "\n",
    "# Example: Count submissions per day\n",
    "gold_df = silver_df.groupBy(\n",
    "    \"submission_time\", \n",
    "    \"location\"  # Replace with specific region or other grouping fields\n",
    ").agg(\n",
    "    count(\"*\").alias(\"total_submissions\"),\n",
    "    avg(\"latitude\").alias(\"avg_latitude\"),\n",
    "    avg(\"longitude\").alias(\"avg_longitude\")\n",
    ")\n",
    "\n",
    "# Example: Filter for regions with more than 100 submissions\n",
    "gold_df = gold_df.filter(col(\"total_submissions\") > 100)\n",
    "\n",
    "# Write to Delta table for Gold Layer\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kobo_gold\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kobo_Ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
