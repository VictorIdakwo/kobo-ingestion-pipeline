{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3b7e36-bb63-498b-801f-19388aacc9a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from koboextractor import KoboExtractor\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Kobo credentials and API setup\n",
    "api_token = '64deabcd2830d55dafc9a765cf2453c41fde07fc'  # Replace with secure storage in production\n",
    "form_id = 'aKhrYSrKMa6Qtfgzvfjnhn'\n",
    "kobo_base_url = 'https://kobo.humanitarianresponse.info/api/v2'\n",
    "\n",
    "# Initialize KoboExtractor\n",
    "kobo = KoboExtractor(api_token, kobo_base_url)\n",
    "\n",
    "# Function to fetch KoboToolbox data and return as a Spark DataFrame\n",
    "def fetch_kobo_to_spark(api_token, form_id, base_url):\n",
    "    try:\n",
    "        # Fetch data from KoboToolbox API\n",
    "        data = kobo.get_data(form_id)\n",
    "\n",
    "        if \"results\" not in data:\n",
    "            raise KeyError(\"'results' key not found in KoboToolbox API response.\")\n",
    "\n",
    "        # Flatten the JSON response\n",
    "        df = pd.json_normalize(data[\"results\"])\n",
    "\n",
    "        # Optional: Convert timestamp fields to datetime if present\n",
    "        if 'submission_time' in df.columns:\n",
    "            df['submission_time'] = pd.to_datetime(df['submission_time'])\n",
    "\n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        return spark_df\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to fetch or process data: {e}\")\n",
    "\n",
    "# Function to ingest data once\n",
    "def ingest_kobo_data(api_token, form_id, base_url):\n",
    "    try:\n",
    "        # Fetch data and convert to Spark DataFrame\n",
    "        spark_df = fetch_kobo_to_spark(api_token, form_id, base_url)\n",
    "\n",
    "        # Fix NullType arrays by casting to known types for Delta compatibility\n",
    "        spark_df = spark_df.withColumn(\n",
    "            \"_attachments\", col(\"_attachments\").cast(ArrayType(StringType()))\n",
    "        ).withColumn(\n",
    "            \"_geolocation\", col(\"_geolocation\").cast(ArrayType(DoubleType()))\n",
    "        ).withColumn(\n",
    "            \"_tags\", col(\"_tags\").cast(ArrayType(StringType()))\n",
    "        ).withColumn(\n",
    "            \"_notes\", col(\"_notes\").cast(ArrayType(StringType()))\n",
    "        )\n",
    "\n",
    "        # Write to Delta table\n",
    "        spark_df.write.mode(\"append\").saveAsTable(\"kobo_submission\")\n",
    "\n",
    "        # Optional: Display a preview of the latest data\n",
    "        display(spark_df.limit(10))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ingestion: {e}\")\n",
    "\n",
    "# Run the ingestion\n",
    "ingest_kobo_data(api_token, form_id, kobo_base_url)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "kobo_ingestion_job.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
